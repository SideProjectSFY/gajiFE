# Gaji Platform: AI Engineer Work Guide ğŸ¤–

**Last Updated**: 2025-11-19  
**Version**: 1.0  
**Engineer**: AI Engineer (Python/FastAPI/Gemini API)

---

## ğŸ“‹ Overview

ì´ ë¬¸ì„œëŠ” **AI Engineer** ì „ìš© ì‘ì—… ê°€ì´ë“œì…ë‹ˆë‹¤. FastAPI AI Service, VectorDB, Gemini API í†µí•©, Prompt Engineering ê´€ë ¨ ëª¨ë“  ì‘ì—…ì´ í¬í•¨ë©ë‹ˆë‹¤.

**ë‹´ë‹¹ ì˜ì—­**:

- FastAPI AI Service (:8000)
- Google Gemini API (2.5 Flash + Embedding)
- VectorDB (ChromaDB/Pinecone)
- Redis (Long Polling task storage)
- Celery (Async task queue)

**í•µì‹¬ ëª©í‘œ**:

- ì²« í† í° ìƒì„± <3ì´ˆ
- VectorDB ì¿¼ë¦¬ <100ms
- Prompt ìƒì„± <500ms

---

## ğŸ¯ Role & Responsibilities

### ì£¼ìš” ì±…ì„

1. **AI Service ê°œë°œ**: FastAPI ê¸°ë°˜ AI ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤
2. **Gemini API í†µí•©**: Text generation + Embedding
3. **VectorDB ê´€ë¦¬**: 5ê°œ collection (passages, characters, locations, events, themes)
4. **Prompt Engineering**: ì‹œë‚˜ë¦¬ì˜¤ â†’ Gemini prompt ë³€í™˜
5. **Performance Optimization**: Caching, Token management

### ê¸°ìˆ  ìŠ¤íƒ

- **Language**: Python 3.11+
- **Framework**: FastAPI, Uvicorn
- **AI**: Google Gemini 2.5 Flash API, Gemini Embedding API (768-dim)
- **VectorDB**: ChromaDB (dev), Pinecone (prod)
- **Cache**: Redis (Long Polling, 600s TTL)
- **Async**: Celery (task queue)

---

## ğŸ“… Day-by-Day Work Schedule

### Day 1-2: FastAPI & VectorDB Setup (9h)

#### Day 1: ê¸°ì´ˆ ì¸í”„ë¼ êµ¬ì¶• (6h)

**Story 0.2: FastAPI AI Service Setup**

**09:00-12:00 (3h): í™˜ê²½ ì„¤ì • & Gemini API**

```bash
# 1. Python í™˜ê²½ ì„¤ì • (1h)
mkdir -p ai-backend/app/{api,services,models,utils}
cd ai-backend
uv init
uv pip install fastapi uvicorn google-generativeai chromadb redis celery httpx

# 2. Gemini API ì„¤ì • (2h)
# .env íŒŒì¼ ìƒì„±
GEMINI_API_KEY=your_api_key_here
GEMINI_MODEL=gemini-2.0-flash-exp
GEMINI_EMBEDDING_MODEL=models/text-embedding-004

# services/gemini_client.py ì‘ì„±
# - Gemini 2.5 Flash í´ë¼ì´ì–¸íŠ¸
# - Retry logic (3íšŒ, exponential backoff)
# - Test: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±
```

**13:00-15:00 (2h): VectorDB ì„¤ì •**

```python
# services/vectordb_client.py
# 5ê°œ collection ìƒì„±:
# - novel_passages (5000+ passages)
# - characters (100+ characters)
# - locations (50+ locations)
# - events (200+ events)
# - themes (30+ themes)

# í…ŒìŠ¤íŠ¸: Collection ìƒì„± ë° ìƒ˜í”Œ ë°ì´í„° ì‚½ì…
```

**16:00-18:00 (2h): FastAPI ì•± êµ¬ì¡°**

```python
# app/main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(title="Gaji AI Service")

# CORS: Spring Boot only
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8080"],
    allow_methods=["*"],
)

# api/health.py
@app.get("/health")
def health_check():
    return {"status": "healthy", "service": "ai-backend"}

# Port 8000 ì‹¤í–‰
# uvicorn app.main:app --reload --port 8000
```

**ì²´í¬í¬ì¸íŠ¸ (Day 1)**:

- [ ] `uvicorn app.main:app --reload` ì‹¤í–‰ ì„±ê³µ
- [ ] `GET /health` ì‘ë‹µ: `{"status": "healthy"}`
- [ ] Gemini API í…ŒìŠ¤íŠ¸ í†µê³¼ (í…ìŠ¤íŠ¸ ìƒì„± 1íšŒ)
- [ ] ChromaDB 5ê°œ collection ìƒì„± í™•ì¸

**ì˜ì¡´ì„±**: âŒ ì—†ìŒ (ë…ë¦½ì  ì‘ì—…)

---

#### Day 2: Redis & Docker í˜‘ì—… (3h)

**19:00-21:00 (2h): Redis ì„¤ì • (ì•¼ê°„ ì‘ì—…)**

```bash
# Redis ì—°ê²° í…ŒìŠ¤íŠ¸
docker run --name gaji-redis -p 6379:6379 -d redis:7

# services/redis_client.py
# - Redis í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# - Long Polling task storage (TTL 600s)
# - Celery ê¸°ë³¸ ì„¤ì •
```

**Story 0.5: Docker Configuration (í˜‘ì—…, 2.5h)**

**13:00-15:30 (2.5h): FastAPI Dockerfile ì‘ì„±**

```dockerfile
# Dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

# docker-compose.yml (ì¼ë¶€)
services:
  vectordb:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"

  redis:
    image: redis:7
    ports:
      - "6379:6379"

  ai-service:
    build: ./ai-backend
    ports:
      - "8000:8000"
    depends_on:
      - vectordb
      - redis
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY}
```

**ì²´í¬í¬ì¸íŠ¸ (Day 2)**:

- [ ] Redis ì—°ê²° ì„±ê³µ
- [ ] `docker-compose up ai-service` ì‹¤í–‰ ì„±ê³µ
- [ ] VectorDB & Redis ì„œë¹„ìŠ¤ healthy

**ì˜ì¡´ì„±**:

- âš ï¸ Backend Engineerì™€ í˜‘ì—… (docker-compose.yml í†µí•©)

---

### Day 3: VectorDB Data Import (3h)

**Story 0.7: VectorDB Data Import**

**09:00-12:00 (3h): Import Script ì‘ì„± & ì‹¤í–‰**

```python
# scripts/import_dataset.py
import argparse
from app.services.vectordb_client import VectorDBClient
from app.services.gemini_client import GeminiClient

def import_novels(dataset_path: str):
    """
    Import 10+ novels to VectorDB
    - novels.json: Novel metadata
    - passages.parquet: 5000+ passages
    - characters.json: 100+ characters
    """
    vectordb = VectorDBClient()
    gemini = GeminiClient()

    # 1. Load dataset
    novels = load_json(f"{dataset_path}/novels.json")
    passages = load_parquet(f"{dataset_path}/passages.parquet")
    characters = load_json(f"{dataset_path}/characters.json")

    # 2. Generate embeddings (batch 100)
    for i in range(0, len(passages), 100):
        batch = passages[i:i+100]
        embeddings = gemini.embed_batch([p['text'] for p in batch])
        vectordb.add_passages(batch, embeddings)

    # 3. Add characters, locations, events, themes
    # ...

    print(f"Import complete: {len(novels)} novels, {len(passages)} passages")

# ì‹¤í–‰
# python scripts/import_dataset.py --dataset-path /data/gutenberg
```

**ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸**:

```python
# scripts/verify_import.py
def verify():
    vectordb = VectorDBClient()

    # Count check
    assert vectordb.count("novel_passages") > 5000
    assert vectordb.count("characters") > 100

    # Semantic search test
    results = vectordb.search("brave protagonist", collection="characters", top_k=5)
    assert len(results) == 5

    print("âœ… VectorDB import verification passed")

# python scripts/verify_import.py
```

**ì²´í¬í¬ì¸íŠ¸ (Day 3)**:

- [ ] Import script ì‹¤í–‰ ì„±ê³µ (10+ novels, 5000+ passages)
- [ ] VectorDB 5ê°œ collection ë°ì´í„° í™•ì¸
- [ ] Semantic search í…ŒìŠ¤íŠ¸ í†µê³¼ ("brave protagonist" â†’ 5 results)
- [ ] PostgreSQL novel metadata ìƒì„± í™•ì¸ (Backendì™€ í˜‘ì—…)

**ì˜ì¡´ì„±**:

- âœ… Story 0.2 (FastAPI & VectorDB) ì™„ë£Œ í•„ìš”
- âš ï¸ Backend Engineerì™€ í˜‘ì—… (Spring Boot API í˜¸ì¶œí•˜ì—¬ PostgreSQL ë©”íƒ€ë°ì´í„° ìƒì„±)

---

### Day 4: ëŒ€ê¸° & ì¤€ë¹„ ì‘ì—… (2h)

**ì¤€ë¹„ ì‘ì—…**: Epic 1, 2 ì„¤ê³„ & ê³„íš

**16:00-18:00 (2h): ë‹¤ìŒ ì£¼ ì‘ì—… ì¤€ë¹„**

- Prompt Engine ì„¤ê³„ (Story 2.1)
- VectorDB ì¿¼ë¦¬ ìµœì í™” ì „ëµ
- Redis ìºì‹± ì „ëµ ì„¤ê³„

---

### Day 5-7: Epic 2 - AI Engine (32h)

#### Day 5: Prompt Engine êµ¬í˜„ (12h)

**Story 2.1: Scenario to Prompt Engine**

**09:00-12:00 (3h): PromptAdapter ì„¤ê³„**

```python
# services/prompt_adapter.py
from typing import Dict, List
from app.models.scenario import BaseScenario
from app.services.vectordb_client import VectorDBClient

class PromptAdapter:
    """
    ì‹œë‚˜ë¦¬ì˜¤ â†’ Gemini prompt ë³€í™˜
    3ê°€ì§€ íƒ€ì… ì§€ì›:
    - CHARACTER_CHANGE
    - EVENT_ALTERATION
    - SETTING_MODIFICATION
    """

    def __init__(self):
        self.vectordb = VectorDBClient()
        self.redis = RedisClient()

    def generate_prompt(self, scenario: BaseScenario) -> Dict:
        """
        Main prompt generation logic
        """
        # 1. VectorDB ì¡°íšŒ
        context = self._fetch_context(scenario)

        # 2. Prompt template ì ìš©
        prompt = self._apply_template(scenario, context)

        # 3. Redis ìºì‹± (1h TTL)
        cache_key = f"prompt:{scenario.id}"
        self.redis.setex(cache_key, 3600, prompt)

        return {
            "prompt": prompt,
            "token_count": len(prompt.split()),
            "temperature": 0.7
        }
```

**13:00-15:00 (2h): VectorDB ì¡°íšŒ ë¡œì§**

```python
def _fetch_context(self, scenario: BaseScenario) -> Dict:
    """
    VectorDBì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
    """
    context = {}

    # 1. Passage ê²€ìƒ‰ (semantic search)
    if scenario.novel_id:
        passages = self.vectordb.search(
            query=scenario.character_name or scenario.event_name,
            collection="novel_passages",
            filter={"novel_id": scenario.novel_id},
            top_k=10
        )
        context["passages"] = passages

    # 2. Character ì •ë³´ ê²€ìƒ‰
    if scenario.character_name:
        characters = self.vectordb.search(
            query=scenario.character_name,
            collection="characters",
            top_k=3
        )
        context["character"] = characters[0]

    # 3. Location/Event ì •ë³´
    # ...

    return context
```

**16:00-21:00 (5h): Prompt Template ì‘ì„± & API**

```python
# templates/prompt_templates.py
CHARACTER_CHANGE_TEMPLATE = """
You are a creative storytelling AI. Generate a compelling "what if" scenario.

**Original Context:**
{passages}

**Character:** {character_name}
**Original Traits:** {original_traits}
**Changed Trait:** {changed_trait}
**Reasoning:** {reasoning}

Generate a 200-word scenario exploring how this character change would alter the story.
"""

# api/prompt.py
from fastapi import APIRouter
from app.services.prompt_adapter import PromptAdapter

router = APIRouter(prefix="/api/prompts")

@router.post("/generate")
async def generate_prompt(scenario_id: str):
    adapter = PromptAdapter()
    # scenario = fetch_scenario_from_backend(scenario_id)
    prompt_data = adapter.generate_prompt(scenario)
    return prompt_data
```

**ì²´í¬í¬ì¸íŠ¸ (Day 5)**:

- [ ] Prompt ìƒì„± API ë™ì‘ (`POST /api/prompts/generate`)
- [ ] VectorDB ì¡°íšŒ <100ms
- [ ] Redis ìºì‹± ë™ì‘ í™•ì¸ (1h TTL)

**ì˜ì¡´ì„±**:

- âœ… Story 0.7 (VectorDB import) ì™„ë£Œ í•„ìš”
- âš ï¸ Story 1.1 (Backend Scenario API) ì™„ë£Œ í›„ í†µí•© ê°€ëŠ¥

---

#### Day 6: Validation System (6h)

**Story 1.3: Scenario Validation System**

**09:00-12:00 (3h): Gemini API ê²€ì¦ ë¡œì§**

```python
# services/scenario_validator.py
from app.services.gemini_client import GeminiClient

class ScenarioValidator:
    """
    ì‹œë‚˜ë¦¬ì˜¤ í’ˆì§ˆ ê²€ì¦ (Gemini API ì‚¬ìš©)
    """

    def __init__(self):
        self.gemini = GeminiClient()
        self.redis = RedisClient()

    def validate(self, scenario_data: Dict) -> Dict:
        """
        í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)
        """
        # 1. Gemini APIë¡œ ê²€ì¦ ìš”ì²­
        validation_prompt = self._build_validation_prompt(scenario_data)
        response = self.gemini.generate(validation_prompt)

        # 2. í’ˆì§ˆ ì ìˆ˜ íŒŒì‹±
        quality_score = self._parse_quality_score(response)
        issues = self._extract_issues(response)
        suggestions = self._extract_suggestions(response)

        # 3. Redis ìºì‹± (5ë¶„ TTL)
        cache_key = f"validation:{scenario_data['id']}"
        self.redis.setex(cache_key, 300, {
            "quality_score": quality_score,
            "issues": issues,
            "suggestions": suggestions
        })

        return {
            "quality_score": quality_score,
            "issues": issues,
            "suggestions": suggestions
        }
```

**13:00-15:00 (2h): Validation API & Redis ìºì‹±**

```python
# api/validation.py
@router.post("/scenarios/validate")
async def validate_scenario(scenario_data: Dict):
    validator = ScenarioValidator()
    result = validator.validate(scenario_data)
    return result
```

**ì²´í¬í¬ì¸íŠ¸ (Day 6)**:

- [ ] Validation API ë™ì‘ (`POST /api/scenarios/validate`)
- [ ] í’ˆì§ˆ ì ìˆ˜ ì •í™•ë„ í™•ì¸ (ìˆ˜ë™ í…ŒìŠ¤íŠ¸ 10ê°œ)
- [ ] Redis ìºì‹± ë™ì‘ (5ë¶„ TTL)

**ì˜ì¡´ì„±**:

- âœ… Epic 0 ì™„ë£Œ (Gemini API ì„¤ì •)

---

#### Day 7: Context Manager & Character Consistency (14h)

**Story 2.2: Context Window Manager**

**09:00-13:00 (4h): Context Manager êµ¬í˜„**

```python
# services/context_manager.py
class ContextManager:
    """
    1M token window ê´€ë¦¬
    """

    def __init__(self):
        self.max_tokens = 1_000_000
        self.redis = RedisClient()

    def manage_context(self, conversation_id: str, messages: List[Dict]) -> List[Dict]:
        """
        Token counting & window sliding
        """
        # 1. Token counting
        total_tokens = sum(self._count_tokens(msg['content']) for msg in messages)

        # 2. Window sliding (ìµœì‹  ë©”ì‹œì§€ ìš°ì„ )
        if total_tokens > self.max_tokens:
            messages = self._slide_window(messages, self.max_tokens)

        # 3. Redis ì €ì¥ (conversation context)
        self.redis.setex(f"context:{conversation_id}", 3600, messages)

        return messages

    def _count_tokens(self, text: str) -> int:
        # Approximate token counting (1 token â‰ˆ 4 chars)
        return len(text) // 4
```

**14:00-18:00 (4h): Story 2.3 - Character Consistency**

```python
# services/character_consistency.py
class CharacterConsistencyTracker:
    """
    ìºë¦­í„° ì¼ê´€ì„± ì¶”ì 
    """

    def __init__(self):
        self.vectordb = VectorDBClient()
        self.gemini = GeminiClient()

    def extract_traits(self, character_name: str, passages: List[str]) -> Dict:
        """
        Trait extraction using Gemini
        """
        # 1. Gemini APIë¡œ trait ì¶”ì¶œ
        prompt = f"Extract personality traits of {character_name} from: {passages}"
        response = self.gemini.generate(prompt)

        # 2. Parse traits (JSON format)
        traits = self._parse_traits(response)

        # 3. VectorDB ì €ì¥ (Triple storage)
        self.vectordb.add(
            collection="character_traits",
            documents=[{
                "character": character_name,
                "trait": trait,
                "evidence": passages
            } for trait in traits]
        )

        return traits
```

**19:00-21:00 (2h): í†µí•© & í…ŒìŠ¤íŠ¸**

```python
# í†µí•© í…ŒìŠ¤íŠ¸
def test_context_manager():
    manager = ContextManager()
    messages = [{"role": "user", "content": "Hello"} for _ in range(1000)]
    result = manager.manage_context("conv-123", messages)
    assert len(result) <= 1_000_000 // 4  # Token limit check

def test_character_consistency():
    tracker = CharacterConsistencyTracker()
    traits = tracker.extract_traits("Elizabeth Bennet", ["passage1", "passage2"])
    assert "prideful" in traits or "intelligent" in traits
```

**ì²´í¬í¬ì¸íŠ¸ (Day 7)**:

- [ ] Context Manager ë™ì‘ (1M token ê´€ë¦¬)
- [ ] Character Consistency ì¶”ì¶œ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] VectorDBì— trait ì €ì¥ í™•ì¸

**ì˜ì¡´ì„±**:

- âœ… Story 2.1 (Prompt Engine) ì™„ë£Œ í•„ìš”

---

### Day 8-9: Epic 4 - Streaming (24h)

#### Day 8: Streaming êµ¬í˜„ (12h)

**Story 4.2: Message Streaming & AI Integration**

**09:00-13:00 (4h): Gemini Streaming êµ¬í˜„**

```python
# services/gemini_streaming.py
from google.generativeai import GenerativeModel

class GeminiStreamingService:
    """
    Gemini 2.5 Flash Streaming
    """

    def __init__(self):
        self.model = GenerativeModel('gemini-2.0-flash-exp')

    async def stream_response(self, prompt: str, conversation_id: str):
        """
        HTTP chunked transfer encoding
        ëª©í‘œ: ì²« í† í° <3ì´ˆ
        """
        # 1. Context Manager í†µí•©
        context = self._load_context(conversation_id)
        full_prompt = self._build_full_prompt(context, prompt)

        # 2. Streaming ì‹œì‘
        response = self.model.generate_content_stream(
            full_prompt,
            generation_config={
                "temperature": 0.7,
                "max_output_tokens": 2048,
            }
        )

        # 3. Chunk-by-chunk yield
        for chunk in response:
            if chunk.text:
                yield chunk.text
```

**14:00-18:00 (4h): Context Manager í†µí•© & Redis Task Storage**

```python
# services/task_manager.py
class TaskManager:
    """
    Long Pollingìš© task ê´€ë¦¬
    """

    def __init__(self):
        self.redis = RedisClient()

    def create_task(self, conversation_id: str, user_message: str) -> str:
        """
        Task ìƒì„± & Redis ì €ì¥
        """
        task_id = f"task:{uuid.uuid4()}"

        # 1. Task ìƒíƒœ ì´ˆê¸°í™”
        task_data = {
            "status": "pending",
            "conversation_id": conversation_id,
            "user_message": user_message,
            "response": "",
            "created_at": datetime.utcnow().isoformat()
        }

        # 2. Redis ì €ì¥ (TTL 600s = 10ë¶„)
        self.redis.setex(task_id, 600, json.dumps(task_data))

        return task_id

    def update_task(self, task_id: str, status: str, response: str = ""):
        """
        Task ìƒíƒœ ì—…ë°ì´íŠ¸
        """
        task_data = json.loads(self.redis.get(task_id))
        task_data["status"] = status
        task_data["response"] += response  # Streaming: append
        self.redis.setex(task_id, 600, json.dumps(task_data))
```

**19:00-21:00 (2h): Streaming API ì—”ë“œí¬ì¸íŠ¸**

```python
# api/chat.py
from fastapi import APIRouter
from fastapi.responses import StreamingResponse

router = APIRouter(prefix="/api/chat")

@router.post("/stream")
async def stream_chat(conversation_id: str, message: str):
    """
    Streaming API
    Return: task_id for Long Polling
    """
    # 1. Task ìƒì„±
    task_manager = TaskManager()
    task_id = task_manager.create_task(conversation_id, message)

    # 2. Background taskë¡œ streaming ì‹œì‘
    asyncio.create_task(process_streaming(task_id, conversation_id, message))

    return {"task_id": task_id}

async def process_streaming(task_id: str, conversation_id: str, message: str):
    """
    Background: Streaming ì²˜ë¦¬
    """
    streaming = GeminiStreamingService()
    task_manager = TaskManager()

    # Update status: processing
    task_manager.update_task(task_id, "processing")

    # Stream & update
    async for chunk in streaming.stream_response(message, conversation_id):
        task_manager.update_task(task_id, "processing", chunk)

    # Complete
    task_manager.update_task(task_id, "completed")
```

**ì²´í¬í¬ì¸íŠ¸ (Day 8)**:

- [ ] Streaming ê¸°ë³¸ ë™ì‘ í™•ì¸
- [ ] ì²« í† í° <3ì´ˆ ìƒì„± (ëª©í‘œ)
- [ ] Redis task ì €ì¥ í™•ì¸ (TTL 600s)

**ì˜ì¡´ì„±**:

- âœ… Story 2.2 (Context Manager) ì™„ë£Œ í•„ìš”

---

#### Day 9: Streaming ì™„ì„± & Testing (12h)

**09:00-13:00 (4h): Streaming API ì™„ì„±**

- Retry ë¡œì§ (3íšŒ, exponential backoff)
- Error handling (Gemini API 429, timeout)
- Performance íŠœë‹ (token optimization)

**14:00-18:00 (4h): Story 2.4 - Testing & Refinement**

```python
# tests/test_scenarios.py
CORE_SCENARIOS = [
    {
        "name": "Pride and Prejudice - Elizabeth modest",
        "character": "Elizabeth Bennet",
        "trait_change": "prideful â†’ modest",
        "expected_outcome": "acceptance of Darcy's first proposal"
    },
    # ... 10ê°œ í•µì‹¬ ì‹œë‚˜ë¦¬ì˜¤
]

def test_core_scenarios():
    adapter = PromptAdapter()
    streaming = GeminiStreamingService()

    for scenario in CORE_SCENARIOS:
        # 1. Prompt ìƒì„±
        prompt_data = adapter.generate_prompt(scenario)

        # 2. Streaming í…ŒìŠ¤íŠ¸
        response = streaming.stream_response(prompt_data["prompt"], "test-conv")

        # 3. ê²€ì¦
        assert "first token time" < 3  # <3ì´ˆ
        assert len(response) > 100  # ìµœì†Œ 100ì

        print(f"âœ… {scenario['name']} passed")
```

**19:00-21:00 (2h): ì„±ëŠ¥ ìµœì í™”**

- VectorDB ì¿¼ë¦¬ ìµœì í™” (batch processing)
- Redis ìºì‹± ì „ëµ ê°œì„ 
- Token counting ìµœì í™”

**ì²´í¬í¬ì¸íŠ¸ (Day 9)**:

- [ ] Story 4.2 ì™„ì„±: Streaming API (<3ì´ˆ ì²« í† í°)
- [ ] Story 2.4 ì™„ì„±: 10ê°œ í•µì‹¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] Performance ëª©í‘œ ë‹¬ì„± í™•ì¸

**ì˜ì¡´ì„±**: âŒ ì—†ìŒ (AI ë…ë¦½ ì‘ì—…)

---

### Day 10-11: Epic 3 ì§€ì› & ì„±ëŠ¥ ìµœì í™” (16h)

#### Day 10: Semantic Search ì¤€ë¹„ (8h)

**Epic 3 ì§€ì› ì¤€ë¹„**

**09:00-13:00 (4h): VectorDB Semantic Search íŠœë‹**

```python
# services/search_service.py
class SearchService:
    """
    Epic 3 Discoveryë¥¼ ìœ„í•œ semantic search
    """

    def __init__(self):
        self.vectordb = VectorDBClient()
        self.gemini = GeminiClient()

    def semantic_search(self, query: str, filters: Dict = None) -> List[Dict]:
        """
        Semantic search with filters
        """
        # 1. Query embedding
        query_embedding = self.gemini.embed(query)

        # 2. VectorDB ê²€ìƒ‰ (threshold 0.7)
        results = self.vectordb.search(
            query_vector=query_embedding,
            collection="novel_passages",
            filter=filters,
            top_k=20,
            score_threshold=0.7
        )

        return results
```

**14:00-18:00 (4h): Story 5.1 - Tree ë°ì´í„° ì§€ì›**

```python
# services/tree_service.py
class TreeService:
    """
    Conversation Tree ìƒì„± ë¡œì§
    """

    def generate_tree_data(self, conversation_id: str) -> Dict:
        """
        ëŒ€í™” ë…¸ë“œ JSON ìƒì„±
        """
        # 1. Conversation ì¡°íšŒ (Backend API)
        messages = self._fetch_messages(conversation_id)

        # 2. Tree êµ¬ì¡° ìƒì„±
        tree = {
            "root": conversation_id,
            "nodes": [],
            "edges": []
        }

        for i, msg in enumerate(messages):
            tree["nodes"].append({
                "id": msg["id"],
                "content": msg["content"],
                "role": msg["role"],
                "timestamp": msg["created_at"]
            })

            if i > 0:
                tree["edges"].append({
                    "from": messages[i-1]["id"],
                    "to": msg["id"]
                })

        return tree
```

**ì²´í¬í¬ì¸íŠ¸ (Day 10)**:

- [ ] Semantic search ì¤€ë¹„ ì™„ë£Œ (threshold 0.7)
- [ ] Tree ë°ì´í„° ìƒì„± ë¡œì§ ê²€ì¦

---

#### Day 11: Hybrid Search & ì„±ëŠ¥ ê²€ì¦ (8h)

**Story 3.6: Advanced Search (Hybrid)**

**09:00-13:00 (4h): Hybrid Search êµ¬í˜„**

```python
# services/hybrid_search.py
class HybridSearchService:
    """
    Keyword + Similarity í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    """

    def hybrid_search(self, query: str, filters: Dict = None) -> List[Dict]:
        """
        1. Keyword search (PostgreSQL, Backend í˜¸ì¶œ)
        2. Similarity search (VectorDB)
        3. ê²°ê³¼ ë³‘í•© (weighted scoring)
        """
        # 1. Keyword search (Backend API í˜¸ì¶œ)
        keyword_results = self._keyword_search(query, filters)

        # 2. Similarity search
        similarity_results = self._semantic_search(query, filters)

        # 3. ë³‘í•© (0.4 * keyword_score + 0.6 * similarity_score)
        merged = self._merge_results(keyword_results, similarity_results)

        return merged[:20]  # Top 20
```

**14:00-18:00 (4h): Epic 3 ì™„ì„± & ì„±ëŠ¥ ê²€ì¦**

- Semantic search <100ms ë‹¬ì„± í™•ì¸
- Hybrid search ì •í™•ë„ í…ŒìŠ¤íŠ¸
- VectorDB ì¸ë±ìŠ¤ ìµœì í™”

**ì²´í¬í¬ì¸íŠ¸ (Day 11)**:

- [ ] Story 3.6 ì™„ì„±: Hybrid Search ë™ì‘
- [ ] Semantic search <100ms (p95)
- [ ] Epic 3 ì™„ì„± (AI íŒŒíŠ¸)

---

### Day 12-13: ì„±ëŠ¥ ìµœì í™” & ë¶€í•˜ í…ŒìŠ¤íŠ¸ (16h)

#### Day 12: ì„±ëŠ¥ ê°œì„  (8h)

**09:00-13:00 (4h): Embedding ìºì‹± ê°•í™”**

```python
# Redis ìºì‹± ì „ëµ ê°œì„ 
# - Embedding ìºì‹±: 90%+ hit rate ëª©í‘œ
# - Prompt ìºì‹±: 1h TTL
# - Validation ìºì‹±: 5ë¶„ TTL
```

**14:00-18:00 (4h): Context Window ë©”ëª¨ë¦¬ ìµœì í™”**

- Token counting ì•Œê³ ë¦¬ì¦˜ ê°œì„ 
- Window sliding ìµœì í™” (ìµœì‹  ë©”ì‹œì§€ ìš°ì„ )

---

#### Day 13: ë¶€í•˜ í…ŒìŠ¤íŠ¸ & ëª¨ë‹ˆí„°ë§ (8h)

**09:00-13:00 (4h): ë¶€í•˜ í…ŒìŠ¤íŠ¸**

```python
# tests/load_test.py
import asyncio
from locust import HttpUser, task

class AIServiceLoadTest(HttpUser):
    @task
    def stream_chat(self):
        self.client.post("/api/chat/stream", json={
            "conversation_id": "test",
            "message": "Hello AI"
        })

# ëª©í‘œ: 1000 concurrent requests
# ì²« í† í° <3ì´ˆ ë‹¬ì„± í™•ì¸
```

**14:00-18:00 (4h): Query ìµœì í™” & ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**

- VectorDB ì¿¼ë¦¬ ìµœì í™” (batch processing)
- Grafana + Prometheus ê¸°ë³¸ ì„¤ì •

**ì²´í¬í¬ì¸íŠ¸ (Day 13)**:

- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: 1000 concurrent
- [ ] ì²« í† í° <3ì´ˆ ë‹¬ì„± (p95)
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ê°€ë™

---

### Day 14: ìµœì¢… ê²€ì¦ & ë¬¸ì„œí™” (8h)

**09:00-13:00 (4h): ìµœì¢… ì„±ëŠ¥ ê²€ì¦**

- Latency ì¸¡ì • (p50, p95, p99)
- Error rate í™•ì¸ (<0.1%)
- Redis hit rate í™•ì¸ (>90%)

**14:00-18:00 (4h): ë¬¸ì„œí™” & ë°°í¬ ì¤€ë¹„**

- FastAPI Swagger ë¬¸ì„œ ì™„ì„± (`/docs`)
- VectorDB schema ë¬¸ì„œ ì‘ì„±
- Gemini API ì‚¬ìš© ê°€ì´ë“œ ì‘ì„±
- ENV ë³€ìˆ˜ ì„¤ì • ë¬¸ì„œ

**ì²´í¬í¬ì¸íŠ¸ (Day 14)**:

- [ ] ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±: ì²« í† í° <3ì´ˆ, VectorDB <100ms
- [ ] ë¬¸ì„œ ì™„ì„±: API Docs, VectorDB Schema, Usage Guide
- [ ] í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ

---

## ğŸš¦ Daily Integration Checkpoints

### ë§¤ì¼ ì˜¤í›„ 6ì‹œ: AI Service í†µí•© í…ŒìŠ¤íŠ¸

**Day 1-2**:

- [ ] FastAPI :8000 ì‹¤í–‰ í™•ì¸
- [ ] Gemini API í…ŒìŠ¤íŠ¸ í†µê³¼ (1íšŒ)
- [ ] VectorDB 5ê°œ collection ìƒì„±

**Day 3-4**:

- [ ] VectorDB import ì™„ë£Œ (10+ novels, 5000+ passages)
- [ ] Semantic search í…ŒìŠ¤íŠ¸ í†µê³¼

**Day 5-7**:

- [ ] Prompt Engine ë™ì‘ (ì‹œë‚˜ë¦¬ì˜¤ â†’ prompt)
- [ ] Validation API ë™ì‘ (í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°)
- [ ] Context Manager ë™ì‘ (1M token)

**Day 8-9**:

- [ ] Streaming API ë™ì‘ (ì²« í† í° <3ì´ˆ)
- [ ] Redis task storage í™•ì¸
- [ ] 10ê°œ í•µì‹¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ í†µê³¼

**Day 10-13**:

- [ ] Semantic search <100ms
- [ ] Hybrid search ë™ì‘
- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸ 1000 concurrent í†µê³¼

**Day 14**:

- [ ] ì „ì²´ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± í™•ì¸
- [ ] ë¬¸ì„œ ì™„ì„±

---

## ğŸ”§ Troubleshooting Guide

### ì´ìŠˆ 1: Gemini API 429 Too Many Requests

**ì›ì¸**: Rate limit ì´ˆê³¼ (15 requests/minute)
**í•´ê²°**:

```python
# Retry logic with exponential backoff
@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10))
def call_gemini_api():
    return gemini.generate_content(prompt)
```

### ì´ìŠˆ 2: VectorDB ì—°ê²° ì‹¤íŒ¨

**ì›ì¸**: ChromaDB ì„œë¹„ìŠ¤ ë¯¸ì‹¤í–‰
**í•´ê²°**:

```bash
docker-compose up vectordb
# Verify: curl http://localhost:8001/api/v1/heartbeat
```

### ì´ìŠˆ 3: Streaming ì²« í† í° >3ì´ˆ

**ì›ì¸**: Contextê°€ ë„ˆë¬´ í¼
**í•´ê²°**:

- Token counting í™•ì¸
- Window sliding ìµœì í™”
- Prompt ê¸¸ì´ ì¶•ì†Œ

### ì´ìŠˆ 4: Redis ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì›ì¸**: ìºì‹± ë°ì´í„° ëˆ„ì 
**í•´ê²°**:

```bash
# Redis maxmemory ì„¤ì •
docker run -d --name redis -e REDIS_MAXMEMORY=2gb -e REDIS_MAXMEMORY_POLICY=allkeys-lru redis:7
```

---

## ğŸ“ˆ Success Metrics (KPIs)

### ì„±ëŠ¥ ëª©í‘œ

- **ì²« í† í° ìƒì„±**: <3ì´ˆ (p95)
- **VectorDB ì¿¼ë¦¬**: <100ms (p95)
- **Prompt ìƒì„±**: <500ms
- **Error Rate**: <0.1% (Retry í›„)

### í’ˆì§ˆ ëª©í‘œ

- **Validation ì •í™•ë„**: >85% (ìˆ˜ë™ ê²€ì¦)
- **Semantic Search ì •í™•ë„**: >80% (top-10 relevance)
- **Redis Hit Rate**: >90%

### í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€

- **Unit Tests**: >80%
- **Integration Tests**: ì£¼ìš” API í”Œë¡œìš°
- **Load Tests**: 1000 concurrent requests

---

## ğŸ“š Documentation Checklist

### API ë¬¸ì„œ (/docs Swagger)

- [ ] `/health` - Health check
- [ ] `POST /api/prompts/generate` - Prompt ìƒì„±
- [ ] `POST /api/scenarios/validate` - Validation
- [ ] `POST /api/chat/stream` - Streaming
- [ ] `GET /api/tasks/{id}` - Task ìƒíƒœ ì¡°íšŒ

### VectorDB Schema

- [ ] 5ê°œ collection êµ¬ì¡° ì„¤ëª…
- [ ] Embedding ìƒì„± ë°©ë²•
- [ ] Query ì˜ˆì œ

### Gemini API Usage Guide

- [ ] API Key ì„¤ì • ë°©ë²•
- [ ] Rate limiting ê°€ì´ë“œ
- [ ] Error handling ê°€ì´ë“œ

---

**Document Owner**: AI Engineer  
**Last Updated**: 2025-11-19  
**Next Review**: Day 7 (Epic 2 ì™„ì„± í›„)
